{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEi7LLkfbFAs"
   },
   "source": [
    "Baseado em https://github.com/aksh-ai/neuralBlack/blob/master/torch_brain_tumor_classifier.ipynb\n",
    "\n",
    "E https://iopscience.iop.org/article/10.1088/1361-6560/aac71a/pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4k9AZlhoVgg"
   },
   "source": [
    "## Import Necessary Libraries\n",
    "\n",
    "We'll be using the following libraries to complete our classification problem:\n",
    "\n",
    "* Numpy - For linear algebra operations \n",
    "* Torch - Pytorch Deep Learning Framework\n",
    "* Torch NN - Neural network class from Pytorch library\n",
    "* Torch NN Functional - Functional Neural Network class from Pytorch library\n",
    "* Torch Utils Data: DataLoader, Dataset - Dataset class used to create custom dataset class by subclassing it and DataLoader is used to laod data in batches using dataset class in real-time.\n",
    "* Torchvision: Transforms, Models - Trochvision provides augmentation techniques using transforms class and transfer learning models are available in models class\n",
    "* OS - To use Operating System methods\n",
    "* Random - To set random seed at specific places where random operations take place just so it happens the same way everytime it is executed\n",
    "* Pandas - To create DataFrame, CSV files, etc\n",
    "* Time - To perform date time operations\n",
    "* Seaborn - For sophisticated visualization\n",
    "* Pickle - To save and load binary files of our training data\n",
    "* Scikit-Learn - For evaluating our Classifier and for cross-validation split\n",
    "* Matplotlib - To visualize images, losses and accuracy\n",
    "* Google Colab Drive - To mount Google Drive so we can perform storage and loading operations using it\n",
    "                      \n",
    "Let's go ahead and import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppY0E2oBcXNu",
    "outputId": "f010cf23-6adc-4e06-ab53-43482d86e659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchio in /home/jonathan/.local/lib/python3.8/site-packages (0.18.57)\n",
      "Requirement already satisfied: tqdm in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from torchio) (4.62.3)\n",
      "Requirement already satisfied: nibabel in /home/jonathan/.local/lib/python3.8/site-packages (from torchio) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from torchio) (1.21.2)\n",
      "Requirement already satisfied: humanize in /home/jonathan/.local/lib/python3.8/site-packages (from torchio) (3.12.0)\n",
      "Requirement already satisfied: scipy in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from torchio) (1.7.1)\n",
      "Requirement already satisfied: torch>=1.1 in /home/jonathan/.local/lib/python3.8/site-packages (from torchio) (1.9.1)\n",
      "Requirement already satisfied: Deprecated in /home/jonathan/.local/lib/python3.8/site-packages (from torchio) (1.2.13)\n",
      "Requirement already satisfied: SimpleITK!=2.0.* in /home/jonathan/.local/lib/python3.8/site-packages (from torchio) (2.1.1)\n",
      "Requirement already satisfied: click in /home/jonathan/.local/lib/python3.8/site-packages (from torchio) (8.0.3)\n",
      "Requirement already satisfied: typing-extensions in /home/jonathan/.local/lib/python3.8/site-packages (from torch>=1.1->torchio) (3.10.0.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/jonathan/.local/lib/python3.8/site-packages (from Deprecated->torchio) (1.13.2)\n",
      "Requirement already satisfied: setuptools in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from humanize->torchio) (58.2.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from nibabel->torchio) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from packaging>=14.3->nibabel->torchio) (2.4.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Invalid requirement: 'torch#==1.9.0'\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in /home/jonathan/.local/lib/python3.8/site-packages (0.10.1)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from torchvision) (8.3.2)\n",
      "Requirement already satisfied: torch==1.9.1 in /home/jonathan/.local/lib/python3.8/site-packages (from torchvision) (1.9.1)\n",
      "Requirement already satisfied: numpy in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from torchvision) (1.21.2)\n",
      "Requirement already satisfied: typing-extensions in /home/jonathan/.local/lib/python3.8/site-packages (from torch==1.9.1->torchvision) (3.10.0.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /home/jonathan/.local/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from seaborn) (1.21.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from seaborn) (3.4.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from seaborn) (1.3.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (8.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/jonathan/.local/lib/python3.8/site-packages (1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jonathan/.local/lib/python3.8/site-packages (from scikit-learn) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from scikit-learn) (1.21.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/jonathan/.local/lib/python3.8/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from scikit-learn) (1.7.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in /home/jonathan/.local/lib/python3.8/site-packages (4.5.3.56)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/JupyterLab/resources/jlab_server/lib/python3.8/site-packages (from opencv-python) (1.21.2)\n",
      "/home/jonathan/Dropbox/UnB/unet_mri_reconstruction\n"
     ]
    }
   ],
   "source": [
    "!pip install torchio\n",
    "!pip install torch#==1.9.0\n",
    "!pip install torchvision\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install opencv-python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from PIL import Image\n",
    "import sys\n",
    "import cv2\n",
    "import torchio as tio\n",
    "\n",
    "import k_space, subsampling, radial\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ_bNRuloctt"
   },
   "source": [
    "Import Google Drive for persistent storage of our training data, neural network model weights and other required files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMpvVU6Tq2jy",
    "outputId": "fa7d2163-61de-426c-8641-5c8e4a792931"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26865/389378830.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshepp_logan_phantom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msampling_pattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "from skimage.data import shepp_logan_phantom\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def sampling_pattern(sampling,imsize):\n",
    "    if sampling==0:\n",
    "        idx=[i for i in range(0,imsize[0]*imsize[1])]\n",
    "    elif sampling==1:\n",
    "        idx=subsampling.vertical_subsampling(imsize, rate=2, low_freq=0)\n",
    "    elif sampling==2:\n",
    "        idx=subsampling.vertical_subsampling(imsize, rate=2, low_freq=12/100)\n",
    "    elif sampling==3:\n",
    "        idx=subsampling.vertical_subsampling(imsize, rate=4, low_freq=0)\n",
    "    elif sampling==4:\n",
    "        idx=subsampling.vertical_subsampling(imsize, rate=4, low_freq=12/100)\n",
    "    elif sampling==5:\n",
    "        idx=radial.radial2D(20,imsize)\n",
    "    elif sampling==6:\n",
    "        idx=radial.radial2D(40,imsize)\n",
    "    elif sampling==7:\n",
    "        idx=radial.radial2D(60,imsize)\n",
    "    elif sampling==8:\n",
    "        idx=radial.radial2D(80,imsize)\n",
    "    elif sampling==9:\n",
    "        idx=radial.radial2D(100,imsize)\n",
    "    else:\n",
    "        return False\n",
    "    return idx\n",
    "    \n",
    "def l2_rec_preset(image,num=2):\n",
    "    imsize=image.shape\n",
    "    idx=sampling_pattern(num,imsize)\n",
    "    rec=k_space.l2_rec(image,idx)\n",
    "    return rec\n",
    "\n",
    "\n",
    "image = shepp_logan_phantom()  \n",
    "imsize=(256,256)\n",
    "image=resize(image,imsize)\n",
    "def all_funs():\n",
    "    funs=[]\n",
    "    plt.figure(figsize = (20,10))\n",
    "\n",
    "    for i in range(10):\n",
    "        f=lambda im,i=i: l2_rec_preset(im,num=i)\n",
    "        funs.append(f)\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(f(image), cmap = 'gray')        \n",
    "    plt.show()\n",
    "    return funs\n",
    "funs=all_funs()\n",
    "    \n",
    "'''\n",
    "MAX=5\n",
    "plus_n=[lambda x: x+i for i in range(MAX)]\n",
    "\n",
    "plus_n_=[]\n",
    "for i in range(MAX):\n",
    "    plus_n_.append(lambda x: x+i)\n",
    "\n",
    "def all_plus_n():\n",
    "    plus_ns=[]\n",
    "    for i in range(MAX):\n",
    "        plus_ns.append(lambda x: x+i)\n",
    "    return plus_ns\n",
    "plus_n__=all_plus_n()    \n",
    "\n",
    "for i in range(len(plus_n)):\n",
    "    print('plus_n[{}]({})={}'.format(i,3,plus_n[i](3)))\n",
    "    print('plus_n_[{}]({})={}'.format(i,3,plus_n_[i](3)))\n",
    "    print('plus_n__[{}]({})={}'.format(i,3,plus_n__[i](3)))\n",
    "    print()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGSmz3DG0lr4",
    "outputId": "16e51b2c-c14d-4a26-caa3-e48d273c4ace"
   },
   "outputs": [],
   "source": [
    "\n",
    "image = shepp_logan_phantom()  \n",
    "imsize=(256,256)\n",
    "image=resize(image,imsize)\n",
    "\n",
    "recs=[]\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "\n",
    "for i,f in enumerate(funs):\n",
    "    rec=f(image)\n",
    "    recs.append(rec)\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(f(image), cmap = 'gray')\n",
    "plt.show()\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Im09E185yBP9"
   },
   "outputs": [],
   "source": [
    "# This values defines the dimensions of the resized image used in this notebook.\n",
    "# It's recommended to use (1,256,256) images, as colab doesn't have enough memory\n",
    "# to process bigger images. Resizing the images to this resolution didn't have \n",
    "# much of an impact in the traning results\n",
    "img_channels = 1\n",
    "img_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjPKpYJTooMG"
   },
   "source": [
    "## Custom Dataset Class\n",
    "\n",
    "Create a custom dataset class that augments each image into 8 different angles: 0, 45, 90, 120, 180, 270, 300, 330 degrees. Fuse it with Pytorch's DataLoader class so data can be loaded, augmented and trained in realtime instead of caching all training samples in memory for augmenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKu3ABIu3E_0"
   },
   "outputs": [],
   "source": [
    "import k_space\n",
    "class L2rec:\n",
    "    def __init__(self, sampling):\n",
    "        self.sampling=sampling\n",
    "\n",
    "    def __call__(self, image):\n",
    "        im=np.array(image)\n",
    "        imsize=im.shape\n",
    "        idx=sampling_pattern(self.sampling,imsize)\n",
    "        rec=k_space.l2_rec(im,idx)\n",
    "        \n",
    "        return Image.fromarray(rec)\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr = f\"{self.__class__.__name__  }(sampling={self.sampling})\"\n",
    "        return repr\n",
    "l=L2rec(1) \n",
    "l.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4PuME0qmFvZ"
   },
   "outputs": [],
   "source": [
    "def defineRecTransform(pattern=0,img_size=(256,256)):\n",
    "  funs=all_funs()\n",
    "  transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),         \n",
    "        transforms.Resize(img_size),\n",
    "        L2rec(sampling=pattern),\n",
    "        transforms.ToTensor()                                 \n",
    "    ])\n",
    "  return transform\n",
    "\n",
    "def defineTransform(img_size=(256,256)):\n",
    "  transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),         \n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor()                                 \n",
    "    ])\n",
    "  return transform\n",
    "\n",
    "class BrainTumorDataset(Dataset):\n",
    "  def __init__(self, dataset, img_size=(256,256), sampling_pattern=2):\n",
    "    self.img_size = img_size\n",
    "    self.dataset = dataset\n",
    "    self.transform = defineTransform()\n",
    "    self.rectransform = defineRecTransform(pattern=sampling_pattern)\n",
    "  def __len__(self):\n",
    "    # return length of image samples    \n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    X, y = self.dataset[idx]\n",
    "    # perform transformations on one instance of X\n",
    "    # Original image as a tensor\n",
    "    data = self.transform(X)\n",
    "    recdata = self.rectransform(X)\n",
    "    # store the images in a list\n",
    "    new_batch = [data]\n",
    "    new_batch = new_batch[:]\n",
    "    new_batch_rec = [recdata]\n",
    "    return (torch.stack(new_batch_rec),torch.stack(new_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKlDZh7VqaJj"
   },
   "source": [
    "Load manually separated dataset with ImageFolder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cpu2Jk0u1Gr"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "dataTreino = ImageFolder('./patientImages/splits/train')\n",
    "dataValid = ImageFolder('./patientImages/splits/valid')\n",
    "dataTeste = ImageFolder('./patientImages/splits/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-waNAXI0bkZ-"
   },
   "source": [
    "Using ImageFolder data to feed our custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnTKHLR5mCeI"
   },
   "outputs": [],
   "source": [
    "train_set = BrainTumorDataset(dataTreino,sampling_pattern=2)\n",
    "valid_set = BrainTumorDataset(dataValid,sampling_pattern=2)\n",
    "test_set = BrainTumorDataset(dataTeste,sampling_pattern=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfWO3u4YqiUP"
   },
   "source": [
    "Print original number of samples in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEL7oQ4Y8NIe",
    "outputId": "e5dfc09e-f605-4b16-e6dd-9872c0295423"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of training samples: {len(train_set)}\")\n",
    "print(f\"Number of validation samples: {len(valid_set)}\")\n",
    "print(f\"Number of testing samples: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0udsS0Qqql3g"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PJu7hjEi0Ij",
    "outputId": "80c40bdf-9ea1-4465-abe5-15f5bdd90c84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9XaKzTZqpW0"
   },
   "source": [
    "Create a DataLoader for each set with batch size of 4 and shuffling enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPuDTtHJ2i3r"
   },
   "outputs": [],
   "source": [
    "# Defines batch size. It's important to keep a low number in train, so the \n",
    "# program doesn't crash for lack of memory\n",
    "train_batch_size = 8\n",
    "valid_batch_size = 8\n",
    "test_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5Eil06jrRUT"
   },
   "outputs": [],
   "source": [
    "train_gen = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True, num_workers=2, drop_last=True)\n",
    "valid_gen = DataLoader(valid_set, batch_size=valid_batch_size, shuffle=True, pin_memory=True, num_workers=2, drop_last=True)\n",
    "test_gen = DataLoader(test_set, batch_size=test_batch_size, shuffle=True, pin_memory=True, num_workers=2, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VE1aH9bJq1HD"
   },
   "source": [
    "Get device to set the training to run on GPU or CPU later based on its availibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ka_a8OAPtNHY",
    "outputId": "b0fcab67-5744-4a83-a7f9-f65c68d11adb"
   },
   "outputs": [],
   "source": [
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHncA4uwrAnB"
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "* Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lq3_lCEttAJk",
    "outputId": "f673f824-8778-4865-e71a-0e92a352873b"
   },
   "outputs": [],
   "source": [
    "class Unet(nn.Module):    \n",
    "    def __init__(self): # imsize=[1,256,256]\n",
    "        super().__init__()\n",
    "        #o = [i + 2*p - k - (k-1)*(d-1)]/s + 1\n",
    "        #o=[256+(2*1)-3-(2*0)]/2 +1=\n",
    "        self.encoder_1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, stride=1, padding=1,dilation=1), #in: 1,256,256, out: 64,256,256\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),            \n",
    "            #maxpooling\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding=1), #in: 64,256,256, out: 64,256,256\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #maxpooling\n",
    "        )\n",
    "        self.encoder_2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, stride=2, padding=1), #in: 64,256,256, out: 64,128,128\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),            \n",
    "            #maxpooling\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1), #in: 64,128,128, out: 128,128,128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #maxpooling\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1), #in:128,128,128, out: 128,128,128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #max\n",
    "        )\n",
    "        #input: 128,64,64\n",
    "        self.encoder_3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, stride=2, padding=1), #in: 128,128,128, out: 128,64,64\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),            \n",
    "            #maxpooling\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1), #in: 128,64,64, out: 256,64,64\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.decoder_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=1, padding=1, output_padding=0), #in: 256,64,64, out: 128,64,64\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 128, 3, stride=2, padding=1, output_padding=1),#in: 128,64,64, out: 128,128,128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder_2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=1, padding=1, output_padding=0), #in: 256,128,128, out: 128,128,128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=1, padding=1, output_padding=0),#in: 128,128,128, out: 64,128,128\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64,64, 3, stride=2, padding=1, output_padding=1),#in: 64,128,128, out: 64,256,256\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder_3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=1, padding=1, output_padding=0), #in: 128,256,256, out: 64,256,256\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 64, 3, stride=1, padding=1, output_padding=0),#in: 64,256,256 out: 64,256,256\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 1, 3, stride=1, padding=1, output_padding=0),#in: 64,256,256 out: 1,256,256\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder_1(x)\n",
    "        x2 = self.encoder_2(x1)\n",
    "        x3 = self.encoder_3(x2)\n",
    "        x4 = self.decoder_1(x3)\n",
    "        x5 = self.decoder_2(torch.cat((x4,x2), 1))\n",
    "        x6 = self.decoder_3(torch.cat((x5,x1), 1))\n",
    "\n",
    "        return x6\n",
    "        \n",
    "def testUnet(gen):\n",
    "  unet = Unet()\n",
    "  print(type(gen))\n",
    "  for data in iter(gen):\n",
    "      x=data[0]\n",
    "      print(type(data))\n",
    "      x=x.view(-1, 1, 256,256)\n",
    "      print('x shape:', x.shape)\n",
    "      x1 = unet.encoder_1(x)\n",
    "      print('x1 shape:', x1.shape)\n",
    "      x2 = unet.encoder_2(x1)\n",
    "      print('x2 shape:', x2.shape)\n",
    "      x3 = unet.encoder_3(x2)\n",
    "      print('x3 shape:', x3.shape)\n",
    "      x4 = unet.decoder_1(x3)\n",
    "      print('x4 shape:', x4.shape)\n",
    "      print('x4 cat x2 shape:', torch.cat((x4,x2), 1).shape)      \n",
    "      x5 = unet.decoder_2(torch.cat((x4,x2), 1))\n",
    "      print('x5 shape:', x5.shape)\n",
    "      print('x5 cat x1 shape:', torch.cat((x5,x1), 1).shape)\n",
    "      x6 = unet.decoder_3(torch.cat((x5,x1), 1))\n",
    "      print('x6 shape:', x6.shape)\n",
    "\n",
    "      break\n",
    "  \n",
    "testUnet(test_gen)\n",
    "\n",
    "unet=Unet()\n",
    "# set all paramters as trainable\n",
    "for param in unet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# set model to run on GPU or CPU absed on availibility\n",
    "unet.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QdrEPESsKQi"
   },
   "source": [
    "## Set Training Configuration\n",
    "\n",
    "* Set model's loss function as MSE\n",
    "\n",
    "* Set RMSprop optimizer with 0.9 weigth decay and learning rate 0.001 as the model's optimizer.\n",
    "\n",
    "* Create empty lists to store training losses, validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cokrpXp3ud8_"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "# if GPU is available set loss function to use GPU\n",
    "#optimizer = torch.optim.Adam(unet.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.RMSprop(unet.parameters(), lr=0.001, weight_decay=0.9)\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# number of training iterations\n",
    "epochs = 2000\n",
    "\n",
    "# empty lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E_rg4eJs7Xq"
   },
   "source": [
    "## Util function\n",
    "\n",
    "### Checkpoint Saver\n",
    "A function to save the model using checkpoints based on best loss achieved during every iteration compared with previous iteration's loss. We'll load the checkpoint and resume training in case Colab's runtime get's disconnected due to inactivity or any other issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnUksQpFNxNK"
   },
   "outputs": [],
   "source": [
    "PATH='./unet_ckpt1.pth.tar'\n",
    "def save_checkpoint(epoch, model, optimizer, loss, best_prec1, train_losses, val_losses, is_best, filename=PATH):\n",
    "    if is_best:\n",
    "        d={ 'epoch': i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'best_prec1': best_prec1\n",
    "          }\n",
    "        torch.save(d, filename)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ci1Lw3UgtU13"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "0TWAlOEau6k7",
    "outputId": "5b75f131-594c-4daa-bf7b-69a1dcc6e8b1"
   },
   "outputs": [],
   "source": [
    "# set training start time\n",
    "start_time = time.time()\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "# set best_prec loss value as 2 for checkpoint threshold\n",
    "best_prec1 = 999999\n",
    "epoch=0\n",
    "\n",
    "if os.path.isfile(PATH):\n",
    "    checkpoint = torch.load(PATH)\n",
    "    unet.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    train_losses= checkpoint['train_losses']\n",
    "    val_losses= checkpoint['val_losses']\n",
    "    best_prec1 = checkpoint['best_prec1']\n",
    "    print('loading checkpoint', PATH)\n",
    "    \n",
    "\n",
    "# empty batch variables\n",
    "b = None\n",
    "train_b = None\n",
    "test_b = None\n",
    "img_channels, img_size=1,256\n",
    "\n",
    "# start training\n",
    "while epoch<epochs:\n",
    "    epoch+=1\n",
    "    print('epoch:', epoch)\n",
    "    unet.train()\n",
    "    # empty training correct and test correct counter as 0 during every iteration\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    \n",
    "    # contador de imageis totais\n",
    "    total_images = 0\n",
    "    \n",
    "    # set epoch's starting time\n",
    "    e_start = time.time()\n",
    "    \n",
    "    # train in batches\n",
    "    for b, (x, y) in enumerate(train_gen):\n",
    "        if b%10==0:\n",
    "            print('{}/{}, time: {:.2f}min'.format(b,len(train_gen),(time.time()-start_time)/60),end='| ')\n",
    "        # set label as cuda if device is cuda\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x=x.view(-1, img_channels, img_size, img_size)\n",
    "        # forward pass image sample\n",
    "        y_pred = unet(x)\n",
    "        \n",
    "        #y_pred=decoder latente)\n",
    "        # calculate loss\n",
    "        loss = criterion(x.float(), y_pred.float())\n",
    "\n",
    "        # set optimizer gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # back propagate with loss\n",
    "        loss.backward()\n",
    "        # perform optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # set epoch's end time\n",
    "    e_end = time.time()\n",
    "    # print training metrics\n",
    "    print(f'Epoch {(i+1)}\\n  Loss: {loss.item():2.4f}  Duration: {((e_end-e_start)/60):.2f} minutes') # total_images = 4 images per batch * 8 augmentations per image * batch length\n",
    "\n",
    "    # some metrics storage for visualization\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    x, y = None, None\n",
    "\n",
    "    # validate using validation generator\n",
    "    # do not perform any gradient updates while validation\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        for b, (x, y) in enumerate(valid_gen):\n",
    "            # set label as cuda if device is cuda\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x=x.view(-1, img_channels, img_size, img_size)\n",
    "            # forward pass image\n",
    "            y_val = unet(x)\n",
    "            #y_val = decoder(latente)\n",
    "            if b==0:\n",
    "              plt.figure(figsize = (10,20))\n",
    "              plt.subplot(1, 2, 1),plt.imshow(x.detach().cpu().numpy()[0,0,:,:], cmap = 'gray')\n",
    "              plt.subplot(1, 2, 2),plt.imshow(y_val.detach().cpu().numpy()[0,0,:,:], cmap = 'gray')\n",
    "              #plt.subplot(1, 3, 3),plt.imshow(y.detach().cpu().numpy()[0,0,:,:], cmap = 'gray')\n",
    "              #plt.title(f'Imagem original (resized {img_size}x{img_size})'), plt.xticks([]), plt.yticks([])\n",
    "              plt.show()\n",
    "              \n",
    "\n",
    "\n",
    "\n",
    "    # get loss of validation set\n",
    "    loss = criterion(x.float(), y_val.float())\n",
    "    # print validation metrics\n",
    "    print(f'Validation Loss: {loss.item():2.4f}\\n')\n",
    "    \n",
    "    # some metrics storage for visualization\n",
    "    test_b  = b\n",
    "    val_losses.append(loss)\n",
    "    \n",
    "    # if current validation loss is less than previous iteration's validatin loss create and save a checkpoint\n",
    "    is_best = loss < best_prec1\n",
    "    best_prec1 = min(loss, best_prec1)\n",
    "    save_checkpoint(i + 1,unet, optimizer, loss, best_prec1, train_losses, val_losses, is_best)\n",
    "\n",
    "\n",
    "\n",
    "# set total training's end time\n",
    "end_time = time.time() - start_time    \n",
    "\n",
    "# print training summary\n",
    "print(\"\\nTraining Duration {:.2f} minutes\".format(end_time/60))\n",
    "print(\"GPU memory used : {} kb\".format(torch.cuda.memory_allocated()))\n",
    "print(\"GPU memory cached : {} kb\".format(torch.cuda.memory_cached()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5hiXkZkwW-W"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rzl2f2kteb8"
   },
   "source": [
    "## Save the model\n",
    "\n",
    "Save the model after the training is completed\n",
    "\n",
    "**Lembre-se de mudar o nome do arquivo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4yzcJMmwPnY"
   },
   "outputs": [],
   "source": [
    "torch.save(unet.state_dict(), './unet_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-2R8Yputlvh"
   },
   "source": [
    "## Evaluation (com imagens sem degradações)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPEBr7E1tobU"
   },
   "source": [
    "Print the validation accuracy of the model calculated using validation set during training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnauSi8Dt04I"
   },
   "source": [
    "Plot the loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TttSopdFvtr0"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.title('Loss Metrics')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEKbGLWsIt6P"
   },
   "source": [
    "Empty out training set and validation set to free up RAM / Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsVJuIwych21"
   },
   "outputs": [],
   "source": [
    "# unet.load_state_dict(torch.load('/content/drive/MyDrive/2021-BRICS/unet_model.pt'))\n",
    "\n",
    "train_gen = None\n",
    "valid_gen = None\n",
    "train_set = None\n",
    "valid_set = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfVhP2dmvdoi"
   },
   "source": [
    "Set model to evaluation mode\n",
    "\n",
    "Calculate loss, correctly classified samples, predicted values, labels and store them in a list using test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZJ7w9ztv1pb"
   },
   "outputs": [],
   "source": [
    "# set model to evaluation mode\n",
    "unet.eval()\n",
    "\n",
    "# perform no gradient updates\n",
    "with torch.no_grad():\n",
    "    # soem metrics storage for visualization and analysis\n",
    "    correct = 0\n",
    "    test_loss = []\n",
    "\n",
    "    # contador de imageis totais\n",
    "    total_images = 0\n",
    "    # perform test set evaluation batch wise\n",
    "    for (x, y) in test_gen:\n",
    "        # set label to use CUDA if available\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x=x.view(-1, img_channels, img_size, img_size)\n",
    "        \n",
    "        # forward pass image sample\n",
    "        y_pred = unet(x)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(x.float(), y_pred.float())\n",
    "\n",
    "        test_loss.append(loss)\n",
    "        \n",
    "print(f\"Test Loss: {test_loss[-1].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqWVj3e_g48Q"
   },
   "source": [
    "#Fim"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "brain_image_reconstruction_unet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
